{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e2e587d-2694-4720-9a0f-27b5a234ede8",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4414430b-242f-47e6-a7ea-3e6db6e1a931",
   "metadata": {},
   "source": [
    "Polynomial functions and kernel functions are both used in machine learning algorithms, particularly in the context of support vector machines (SVMs) and kernel methods. They are related in that kernel functions can be used to transform data into higher-dimensional spaces, effectively allowing linear algorithms like SVMs to capture non-linear patterns in the data. Polynomial functions are a specific type of kernel function.\n",
    "\n",
    "Here's a more detailed explanation of their relationship:\n",
    "\n",
    "1. Kernel Functions:\n",
    "   - In machine learning, a kernel function is a mathematical function that computes the similarity or inner product between pairs of data points in a high-dimensional feature space.\n",
    "   - Kernel functions are primarily used in kernel methods like Support Vector Machines (SVMs) to map data points into a higher-dimensional space without explicitly computing the transformations.\n",
    "   - The choice of kernel function can significantly impact the performance of SVMs and other kernel-based algorithms.\n",
    "   - Common types of kernel functions include linear kernels, polynomial kernels, radial basis function (RBF) kernels, and more.\n",
    "\n",
    "2. Polynomial Functions as Kernel Functions:\n",
    "   - Polynomial kernels are a specific type of kernel function.\n",
    "   - The polynomial kernel of degree 'd' is defined as K(x, y) = (x • y + c)^d, where 'x' and 'y' are data points, 'c' is a constant, and 'd' is the degree of the polynomial.\n",
    "   - The polynomial kernel effectively maps data points into a higher-dimensional space where non-linear patterns may become linearly separable.\n",
    "   - By adjusting the degree 'd' and the constant 'c' in the polynomial kernel, you can control the complexity of the transformation and the model's ability to capture non-linear relationships in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f8c127-53ee-4fe7-9478-f8135496f509",
   "metadata": {},
   "source": [
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e02158-b773-418a-acfb-12c6529ec0ac",
   "metadata": {},
   "source": [
    "You can implement a Support Vector Machine (SVM) with a polynomial kernel in Python using Scikit-learn (also known as sklearn). Scikit-learn provides a convenient library for various machine learning algorithms, including SVMs. Here's a step-by-step guide on how to do it:\n",
    "\n",
    "1. Import Required Librares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bfb0cd1-f560-40be-a4a8-d5e876d72237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892e4d6-bb4b-439e-9fb3-de3686b255bd",
   "metadata": {},
   "source": [
    "2. Load Your Dataset: You need to load your dataset using the datasets module or by reading your data from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d3494a2-a611-4284-b122-84f7e1d059c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b70da-3a70-49a7-84e5-4ef33288b197",
   "metadata": {},
   "source": [
    "3. Split the Data into Training and Testing Sets: It's essential to split your dataset into a training set and a testing set to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92aa48c8-bc01-4763-8bce-f5afdd8e4776",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221f39d3-fd3d-4f12-ae84-4759bc199acf",
   "metadata": {},
   "source": [
    "4. Create and Train the SVM Model: Create an instance of the SVC class (Support Vector Classification) with the kernel parameter set to 'poly' for a polynomial kernel. You can also specify other hyperparameters like the degree of the polynomial kernel (degree), regularization parameter (C), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3fa925f-a256-4407-a741-4b7664e3c44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = SVC(kernel='poly', degree=3, C=1.0) \n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779619d9-c10b-4b6a-9274-4456ac4939fb",
   "metadata": {},
   "source": [
    "5. Make Predictions: Use the trained SVM model to make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03c413cd-a2e2-4774-bd54-1c2f82f5f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ec89e-afa5-4e5f-a6f2-45a2918afe2d",
   "metadata": {},
   "source": [
    "6. Evaluate the Model: You can evaluate the model's performance using metrics like accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "265ab6aa-187d-471b-bf89-4ae08bc4edfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff87d5-de99-4986-8174-fc83c16e4acc",
   "metadata": {},
   "source": [
    "7. Tune Hyperparameters: Depending on your dataset and problem, you may need to fine-tune hyperparameters like the degree of the polynomial kernel, the regularization parameter 'C', and others to optimize the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db567af-a468-4a47-a5d4-138a506357d8",
   "metadata": {},
   "source": [
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed4b7c-c8f9-42f9-be90-a780d7fe1099",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), the parameter epsilon (ε) is a key hyperparameter that controls the width of the margin around the predicted values within which no penalty is incurred. This margin is sometimes referred to as the \"epsilon-insensitive tube.\" The choice of epsilon has a significant impact on the number of support vectors in SVR:\n",
    "\n",
    "1. Small Epsilon (ε):\n",
    "   - When you set a small value for epsilon, it means you have a narrow margin, and the SVR model aims to fit the training data more closely.\n",
    "   - A narrow margin means that only data points that are very close to the predicted function will be considered as support vectors.\n",
    "   - As a result, with a small epsilon, you are likely to have fewer support vectors because the model can fit the data more precisely without needing to consider many data points as support vectors.\n",
    "\n",
    "2. Large Epsilon (ε):\n",
    "   - When epsilon is set to a larger value, you create a wider margin around the predicted function, which allows for more data points to be within the margin without incurring a penalty.\n",
    "   - With a wider margin, the SVR model can be less sensitive to individual data points and may include more data points as support vectors.\n",
    "   - Consequently, a larger epsilon tends to lead to a larger number of support vectors because the margin is more permissive, and more data points fall within it.\n",
    "\n",
    "The value of epsilon in SVR controls the trade-off between model accuracy and simplicity. A smaller epsilon aims for a more accurate fit to the training data but may lead to fewer support vectors, while a larger epsilon allows for a wider margin and more support vectors, resulting in a smoother but potentially less accurate fit. The choice of epsilon should be based on your specific problem and the desired balance between model complexity and generalization. It often requires experimentation and cross-validation to determine the optimal epsilon value for a given regression task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a457b2f6-e6a2-4845-823e-ef4f920cd9bf",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5861d998-30c9-4292-adaa-ec190c9d5c22",
   "metadata": {},
   "source": [
    "1. Kernel Function: The kernel function is used to transform the input features into a high-dimensional space where the hyperplane can be constructed. The choice of kernel function depends on the nature of the data and the problem at hand. LibSVM provides several kernel functions, including linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "\n",
    "    - Linear Kernel: The linear kernel is the simplest kernel function, and it is used when the data is linearly separable. It can be defined as K(x, y) = x^T y, where x and y are the input feature vectors.\n",
    "    - Polynomial Kernel: The polynomial kernel is used when the data is not linearly separable. It can be defined as K(x, y) = (gamma * x^T y + coef0)^degree, where gamma, coef0, and degree are the kernel parameters.\n",
    "    - RBF Kernel: The radial basis function kernel is the most popular kernel function, and it is used when the data is not linearly separable. It can be defined as K(x, y) = exp(-gamma * ||x - y||^2), where gamma is the kernel parameter.\n",
    "    - Sigmoid Kernel: The sigmoid kernel is used when the data is not linearly separable. It can be defined as K(x, y) = tanh(gamma * x^T y + coef0), where gamma and coef0 are the kernel parameters.\n",
    "    \n",
    "When to Increase/Decrease Kernel Complexity:\n",
    "- Increase kernel complexity (e.g., use a higher-degree polynomial or a smaller gamma) when the data is more complex and you want the model to capture intricate patterns.\n",
    "- Decrease kernel complexity when you want to simplify the model and prevent overfitting on noisy data.\n",
    "\n",
    "\n",
    "2. C Parameter: The C parameter controls the trade-off between maximizing the margin and minimizing the error. A larger value of C will result in a smaller margin but a more accurate fit to the training data. Conversely, a smaller value of C will result in a larger margin but a less accurate fit to the training data.\n",
    "\n",
    "When to Increase/Decrease C:\n",
    "- Increase C when you have high confidence in your data or want to reduce training errors.\n",
    "- Decrease C when you want a wider margin or believe some training errors are acceptable to improve generalization.\n",
    "\n",
    "3. Epsilon Parameter: The epsilon parameter defines the width of the margin. It is used to control the sensitivity of the model to errors in the training data. A larger value of epsilon will result in a wider margin and a more robust model, but it may also lead to underfitting. Conversely, a smaller value of epsilon will result in a narrower margin and a more sensitive model, but it may also lead to overfitting.\n",
    "\n",
    "When to Increase/Decrease Epsilon:\n",
    "- Increase epsilon when you want to tolerate larger errors in your predictions and prioritize smoother fits.\n",
    "- Decrease epsilon when you want to minimize errors and obtain a closer fit to the training data.\n",
    "\n",
    "4. Gamma Parameter: The gamma parameter is used only for the RBF kernel function. It controls the width of the Gaussian distribution used to compute the similarity between the input features. A larger value of gamma will result in a narrower Gaussian distribution and a more complex model, but it may also lead to overfitting. Conversely, a smaller value of gamma will result in a wider Gaussian distribution and a simpler model, but it may also lead to underfitting.\n",
    "\n",
    "When to Increase/Decrease Gamma:\n",
    "- Increase gamma when you have confidence that the target variable is influenced by nearby data points and you want the model to capture fine details.\n",
    "- Decrease gamma when you want the model to have a more global perspective and be less sensitive to individual data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc4916-c6c4-45b8-84d9-33a7de82fde5",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "- Import the necessary libraries and load the dataset\n",
    "- Split the dataset into training and testing set\n",
    "- Preprocess the data using any technique of your choice (e.g. scaling, normalization)\n",
    "- Create an instance of the SVC classifier and train it on the training data\n",
    "- Use the trained classifier to predict the labels of the testing data\n",
    "- Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-score)\n",
    "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performance\n",
    "- Train the tuned classifier on the entire dataset\n",
    "- Save the trained classifier to a file for future use.\n",
    "\n",
    "Note:You can use any dataset of your choice for this assignment, but make sure it is suitable for classification and has a sufficient number of features and samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f574f2b8-9b0c-4402-864a-950c7e6a565f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries and load the dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "# Load the dataset (Iris dataset as an example)\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Preprocess the data by scaling it\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create an instance of the SVC classifier and train it on the training data\n",
    "svc = SVC(kernel='rbf', C=1.0, gamma='scale')  # You can choose different hyperparameters\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 5: Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Step 6: Evaluate the classifier's performance using accuracy as the metric\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Step 7: Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto', 0.1, 1]}\n",
    "grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_svc = grid_search.best_estimator_\n",
    "\n",
    "# Step 8: Train the tuned classifier on the entire dataset\n",
    "best_svc.fit(X, y)\n",
    "\n",
    "# Step 9: Save the trained classifier to a file for future use\n",
    "pickle.dump(best_svc, open('svm_classifier.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12104df-23e1-4d92-8f88-43745970f61c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
